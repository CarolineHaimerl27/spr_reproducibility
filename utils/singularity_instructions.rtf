{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;
\red0\green0\blue0;\red27\green31\blue35;\red194\green11\blue35;\red160\green169\blue180;\red244\green246\blue249;
\red238\green238\blue238;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c0\c0;\csgray\c100000;
\csgray\c0\c0;\cssrgb\c14118\c16078\c18431;\cssrgb\c81176\c13333\c18039;\cssrgb\c68627\c72157\c75686\c20000;\cssrgb\c96471\c97255\c98039;
\cssrgb\c94831\c94831\c94831;\cssrgb\c0\c0\c0\c79299;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 These are instructions adapted from Shenglong to create a singularity container that can run torch code.  The super short version: Singularity is like a virtual environment that has exactly reproducible computing environment. Running a singularity command opens up a little self-contained file system and computing environment for you to do stuff. We are going to build one up, the interactively runs. Job on the cluster to test the spr code\
\
I have ben keeping my singularity images in the same subfolder for easy finding: /scratch/dh148/singularity. Make this\
\
\cf2 \cb3 user=dh148 #change this to your netid\cf0 \cb1 \
\cb2 \
\cf2 \cb3 cd /scratch/$user/singularity \
mkdir -p  singularity\
cd singularity \cf0 \cb1 \
\
- copy and unzip the singularity overlay image. This will be mounted on /ext3 when you go into the singularity container.\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf4 \cb3 \CocoaLigature0 cp /scratch/work/public/overlay-fs-ext3/overlay-10GB-400K.ext3.gz ./\CocoaLigature1 \
\CocoaLigature0 gunzip overlay-10GB-400K.ext3.gz\
\cf3 \cb5 \
Find the right singularity environment that will support pytorch and Cuda. It is /scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif\
\
For tensorflow is it /scratch/work/public/singularity/cuda11.0-cudnn8-devel-ubuntu18.04.sif  (this might be outdated now) \cf0 \cb1 \CocoaLigature1 \
\
-\
To setup conda environment, fist launch container interactively, download conda, and make a script to load it on starting the singularity environment. This is the basic structure of the command to start singularity:\
\
\cf4 \cb3 singularity exec --overlay \CocoaLigature0 overlay-10GB-400K.ext3.gz\CocoaLigature1  /scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif /bin/bash\cf0 \cb1 \
\
-Now inside the container, download and install miniconda into /ext3/miniconda3\
\
\cf2 \cb3 wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\
sh Miniconda3-latest-Linux-x86_64.sh -b -p /ext3/miniconda3\cf0 \cb1 \
\
-Create a wrapper script with editors such as vi or emacs, /ext3/env.sh \
\'97\'97\'97\
#!/bin/bash\
source /ext3/miniconda3/etc/profile.d/conda.sh\
export PATH=/ext3/miniconda3/bin:$PATH\
\'97\'97\'97\
\
-To active base conda\
\
\cf2 \cb3 source /ext3/env.sh\cf0 \cb1 \
\
\
-To test, see if these commands point to files. If pip does NOT, then chat with me:\
\
\cf4 \cb3  which conda\
 which python\
 which pip\cf0 \cb1 \
\
\
- now let\'92s install the files needed for the code: pytorch, str, atari-py, and the atari rams\
\
\pard\pardeftab720\partightenfactor0
\cf4 \cb3 \expnd0\expndtw0\kerning0
conda install pytorch torchvision -c pytorch\
export LC_ALL=C.UTF-8\
export LANG=C.UTF-8\
\
cd /scratch/$user\
git clone https://github.com/mila-iqia/spr\
cd spr\
pip install -r requirements.txt\
\pard\pardeftab720\sa320\partightenfactor0
\cf4 pip install atari-py\cf6 \cb8 \
\cf6 \cb5 Get the ROMS and an archiving tool to open them\cb9 \
\pard\pardeftab720\partightenfactor0
\cf4 \cb3 \kerning1\expnd0\expndtw0 \CocoaLigature0 wget http://www.atarimania.com/roms/Roms.rar\expnd0\expndtw0\kerning0
\CocoaLigature1  \
\kerning1\expnd0\expndtw0 \CocoaLigature0 conda install -c conda-forge go-archiver\
arc unarchive Roms.rar\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf4 python -m atari_py.import_roms Roms\
\
\cf3 \cb5 \
Now try and run the code. First exit the container, and rename it\
\cf10 \cb11 Exit\
mv overlay-10GB-400K.ext3 spr.ext3\
\
\cf3 \cb5 \
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
\
\
Ok, now let\'92s try running an interactive job on the cluster and opening up the singularity container. From there, we can try the spr code\
\
\cf10 \cb11 srun --nodes=1 --ntasks-per-node=1 --time=8:00:00 --mem=20GB --gres=gpu /bin/bash\
\
\cf3 \cb5 Once that is running, open up a new terminal on greene, and ssh onto that node. We will open up a singularity container from there\cf4 \CocoaLigature1 \
\cf0 \cb1 \
\cf10 \cb11 \CocoaLigature0 user=dh148\
ovfile=/scratch/$user/singularity/spr.ext3\cf0 \cb1 \CocoaLigature1 \
\cf10 \cb11 \CocoaLigature0 singfile=/scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif\
singularity exec --nv   --overlay $ovfile:ro $singfile /bin/bash\
\cf3 \cb5 \
Once you are the container, try running the SPR code:\
\cf10 \cb11 source /ext3/env.sh\cf0 \cb1 \CocoaLigature1 \
\cf10 \cb11 \CocoaLigature0 cd /scratch/$user/spr\
\pard\pardeftab720\partightenfactor0
\cf4 \cb3 \expnd0\expndtw0\kerning0
\CocoaLigature1 python -m scripts.run --public --game pong --momentum-tau 1.\cf0 \cb1 \kerning1\expnd0\expndtw0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
If you want to run a job as usual with the SLURM scheduler, then make a .sh script that calls your code from inside of a singularity command. Example: run-test.sh. Note that there is an option to keep the overlay as readonly. This has to be done in order to allow multiple processes to use the same overlay (i.e., to have many batch scripts use the same files). We will run a file, torch-test.py\
\
#!/bin/env python\
\
import torch\
\
print(torch.__file__)\
print(torch.__version__)\
\
# How many GPUs are there?\
print(torch.cuda.device_count())\
\
# Get the name of the current GPU\
print(torch.cuda.get_device_name(torch.cuda.current_device()))\
\
# Is PyTorch using a GPU?\
print(torch.cuda.is_available())\
\
\
\
\
This is the SLURM batch script\
\
#!/bin/bash\
\
#SBATCH --nodes=1\
#SBATCH --ntasks-per-node=1\
#SBATCH --cpus-per-task=1\
#SBATCH --time=1:00:00\
#SBATCH --mem=2GB\
#SBATCH --gres=gpu\
#SBATCH --job-name=torch\
\
module purge\
\
ovfile=/scratch/$user/singularity/spr.ext3\
singfile=/scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif\
\
singularity exec --nv \\\
	    --overlay $ovfile:ro $singfile /bin/bash -c "source /ext3/env.sh; python torch-test.py"\
\
\
\
}